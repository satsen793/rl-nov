{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff78fb7",
   "metadata": {},
   "source": [
    "# Robustness Evaluation: Multi-Seed Aggregation for PETS vs MBPO\n",
    "**How to use**\n",
    "1) Place your per-seed CSVs in `/mnt/data/` with the following names:\n",
    "- PETS policy logs: `policy_logs_pets_seed{seed}.csv`\n",
    "- PETS events:      `events_pets_seed{seed}.csv`\n",
    "- MBPO policy logs: `policy_logs_mbpo_seed{seed}.csv`\n",
    "- MBPO events:      `events_mbpo_seed{seed}.csv`\n",
    "\n",
    "2) If you only have a single-seed run, you may still run this notebook; it will compute bootstrap CIs across learners and provide a stability note.\n",
    "\n",
    "**Metrics reported**: Time-to-Mastery (↓), Post-Content Gain (↑), Cumulative Reward (↑), Reward Variance (↓), Blueprint Deviation (↓)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6466be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found seeds -> PETS: [], MBPO: [], Intersect: []\n",
      "No multi-seed files found; falling back to single-seed mode using policy_logs.csv and events.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, glob, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import scipy.stats as st\n",
    "\n",
    "data_dir = Path('/mnt/data')\n",
    "\n",
    "# Discover seeds\n",
    "pets_pl = sorted(data_dir.glob('policy_logs_pets_seed*.csv'))\n",
    "mbpo_pl = sorted(data_dir.glob('policy_logs_mbpo_seed*.csv'))\n",
    "\n",
    "def seeds_from_paths(paths):\n",
    "    seeds = []\n",
    "    for p in paths:\n",
    "        s = int(p.stem.split('seed')[-1])\n",
    "        seeds.append(s)\n",
    "    return sorted(seeds)\n",
    "\n",
    "pets_seeds = seeds_from_paths(pets_pl)\n",
    "mbpo_seeds = seeds_from_paths(mbpo_pl)\n",
    "seeds = sorted(set(pets_seeds).intersection(mbpo_seeds))\n",
    "print(f\"Found seeds -> PETS: {pets_seeds}, MBPO: {mbpo_seeds}, Intersect: {seeds}\")\n",
    "\n",
    "# Fallback: use single-seed (baseline) if multi-seed files are absent\n",
    "single_seed_mode = False\n",
    "if not seeds:\n",
    "    print(\"No multi-seed files found; falling back to single-seed mode using policy_logs.csv and events.csv.\")\n",
    "    single_seed_mode = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3339986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_to_mastery(events: pd.DataFrame, threshold=0.8) -> float:\n",
    "    \"\"\"\n",
    "    Approximate time-to-mastery proxy using rolling correctness on same LO.\n",
    "    Here we compute the median step index where a learner achieves >= threshold over a 5-item window.\n",
    "    \"\"\"\n",
    "    events = events.copy()\n",
    "    events['ts'] = pd.to_datetime(events['ts'])\n",
    "    events = events.sort_values('ts')\n",
    "    ans = events[events['type']=='answered'].copy()\n",
    "    if ans.empty:\n",
    "        return np.nan\n",
    "    ans['correct'] = ans['payload_json'].apply(lambda s: json.loads(s)['correct']).astype(int)\n",
    "    ans['step'] = np.arange(len(ans))\n",
    "    # Rolling mean correctness (window=5)\n",
    "    ans['roll5'] = ans['correct'].rolling(5, min_periods=5).mean()\n",
    "    reached = ans[ans['roll5']>=threshold]\n",
    "    if reached.empty:\n",
    "        return np.nan\n",
    "    return reached['step'].min()\n",
    "\n",
    "def post_content_gain(policy: pd.DataFrame, events: pd.DataFrame, k=3) -> float:\n",
    "    \"\"\"\n",
    "    Measure avg correctness over next-k answers after a content recommendation; minus prior 3 correctness.\n",
    "    \"\"\"\n",
    "    events = events.copy()\n",
    "    events['ts'] = pd.to_datetime(events['ts'])\n",
    "    policy = policy.copy()\n",
    "    policy['ts'] = pd.to_datetime(policy['ts'])\n",
    "    \n",
    "    def next_k_correct_fraction(row, k=3):\n",
    "        sub = events[(events['learner_id']==row['learner_id']) & (events['lo_id']==row['lo_id']) & (events['ts']>row['ts'])]\n",
    "        sub = sub[sub['type']=='answered'].head(k)\n",
    "        if len(sub)==0: \n",
    "            return np.nan\n",
    "        corr = sub['payload_json'].apply(lambda s: json.loads(s)['correct']).astype(int).mean()\n",
    "        return corr\n",
    "\n",
    "    def prev_k_correct_fraction(row, k=3):\n",
    "        sub = events[(events['learner_id']==row['learner_id']) & (events['lo_id']==row['lo_id']) & (events['ts']<row['ts'])]\n",
    "        sub = sub[sub['type']=='answered'].tail(k)\n",
    "        if len(sub)==0: \n",
    "            return np.nan\n",
    "        corr = sub['payload_json'].apply(lambda s: json.loads(s)['correct']).astype(int).mean()\n",
    "        return corr\n",
    "\n",
    "    # only rows where action is a content id\n",
    "    policy_c = policy.dropna(subset=['action_id']).copy()\n",
    "    policy_c['nextk'] = policy_c.apply(next_k_correct_fraction, axis=1, k=k)\n",
    "    policy_c['prevk'] = policy_c.apply(prev_k_correct_fraction, axis=1, k=k)\n",
    "    gain = (policy_c['nextk'] - policy_c['prevk']).dropna()\n",
    "    return float(gain.mean()) if len(gain)>0 else np.nan\n",
    "\n",
    "def cumulative_reward(policy: pd.DataFrame) -> float:\n",
    "    return float(policy['reward'].sum())\n",
    "\n",
    "def reward_variance(policy: pd.DataFrame) -> float:\n",
    "    return float(policy['reward'].var())\n",
    "\n",
    "def blueprint_deviation(events: pd.DataFrame, target=(0.2,0.6,0.2)) -> float:\n",
    "    ev = events[events['type']=='question_shown'].copy()\n",
    "    if ev.empty: return np.nan\n",
    "    ev['difficulty'] = ev['payload_json'].apply(lambda s: json.loads(s)['difficulty'])\n",
    "    obs = ev['difficulty'].value_counts(normalize=True)\n",
    "    obs_vec = np.array([obs.get('Easy',0.0), obs.get('Medium',0.0), obs.get('Hard',0.0)])\n",
    "    tgt = np.array(target)\n",
    "    return float(np.abs(obs_vec - tgt).sum())  # L1 deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c139d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics_for_seed(policy_path: str, events_path: str) -> dict:\n",
    "    pol = pd.read_csv(policy_path)\n",
    "    ev  = pd.read_csv(events_path)\n",
    "    # compute metrics\n",
    "    ttm = time_to_mastery(ev)\n",
    "    pcg = post_content_gain(pol, ev, k=3)\n",
    "    cre = cumulative_reward(pol)\n",
    "    rvar= reward_variance(pol)\n",
    "    bdev= blueprint_deviation(ev)\n",
    "    return {'time_to_mastery': ttm, 'post_content_gain': pcg, 'cum_reward': cre, 'reward_var': rvar, 'blueprint_dev': bdev}\n",
    "\n",
    "def agg_stats(arr: np.ndarray) -> dict:\n",
    "    arr = np.array(arr, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if len(arr)==0:\n",
    "        return {'mean': np.nan, 'sd': np.nan, 'n': 0, 'ci_low': np.nan, 'ci_high': np.nan}\n",
    "    mean = float(arr.mean())\n",
    "    sd   = float(arr.std(ddof=1)) if len(arr)>1 else 0.0\n",
    "    n    = int(len(arr))\n",
    "    se   = sd/np.sqrt(n) if n>1 else 0.0\n",
    "    ci   = (mean - 1.96*se, mean + 1.96*se)\n",
    "    return {'mean': mean, 'sd': sd, 'n': n, 'ci_low': ci[0], 'ci_high': ci[1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a2da27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "if not single_seed_mode:\n",
    "    for s in seeds:\n",
    "        pets_pol = data_dir / f'policy_logs_pets_seed{s}.csv'\n",
    "        pets_ev  = data_dir / f'events_pets_seed{s}.csv'\n",
    "        mbpo_pol = data_dir / f'policy_logs_mbpo_seed{s}.csv'\n",
    "        mbpo_ev  = data_dir / f'events_mbpo_seed{s}.csv'\n",
    "        mp = compute_metrics_for_seed(pets_pol, pets_ev)\n",
    "        mm = compute_metrics_for_seed(mbpo_pol, mbpo_ev)\n",
    "        mp['alg']='PETS'; mp['seed']=s\n",
    "        mm['alg']='MBPO'; mm['seed']=s\n",
    "        results.extend([mp,mm])\n",
    "else:\n",
    "    # single-seed fallback using baseline names\n",
    "    pets_pol = data_dir / 'policy_logs.csv'\n",
    "    pets_ev  = data_dir / 'events.csv'\n",
    "    if pets_pol.exists() and pets_ev.exists():\n",
    "        mp = compute_metrics_for_seed(pets_pol, pets_ev)\n",
    "        mp['alg']='BASELINE'; mp['seed']=0\n",
    "        results.append(mp)\n",
    "\n",
    "res = pd.DataFrame(results)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd259dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired stats unavailable (need multi-seed PETS & MBPO).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paired comparison if we have both algorithms\n",
    "if not single_seed_mode and len(res['seed'].unique())>0:\n",
    "    rows = []\n",
    "    for metric in ['time_to_mastery','post_content_gain','cum_reward','reward_var','blueprint_dev']:\n",
    "        wide = res.pivot(index='seed', columns='alg', values=metric).dropna()\n",
    "        if {'PETS','MBPO'}.issubset(wide.columns):\n",
    "            diff = wide['MBPO'] - wide['PETS']\n",
    "            mean = diff.mean(); sd = diff.std(ddof=1); n=len(diff); se = sd/np.sqrt(n) if n>1 else 0.0\n",
    "            t,p = st.ttest_rel(wide['MBPO'], wide['PETS']) if n>1 else (np.nan, np.nan)\n",
    "            d = mean / sd if (sd>1e-9) else np.nan  # Cohen's d (paired)\n",
    "            ci = (mean - 1.96*se, mean + 1.96*se)\n",
    "            rows.append({'metric':metric, 'mean_diff':mean, 'ci_low':ci[0], 'ci_high':ci[1], 'p_value':p, 'cohen_d':d, 'n':n})\n",
    "    stats_df = pd.DataFrame(rows)\n",
    "    display(stats_df)\n",
    "else:\n",
    "    print(\"Paired stats unavailable (need multi-seed PETS & MBPO).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7e265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate table (mean±sd with 95% CI) per algorithm\n",
    "if len(res)>0:\n",
    "    rows = []\n",
    "    for alg in sorted(res['alg'].unique()):\n",
    "        sub = res[res['alg']==alg]\n",
    "        for metric in ['time_to_mastery','post_content_gain','cum_reward','reward_var','blueprint_dev']:\n",
    "            st_ = agg_stats(sub[metric].values)\n",
    "            rows.append({'alg':alg,'metric':metric, **st_})\n",
    "    agg_df = pd.DataFrame(rows)\n",
    "    display(agg_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f27dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\%'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\%'\n",
      "/tmp/ipykernel_7829/664589677.py:10: SyntaxWarning: invalid escape sequence '\\%'\n",
      "  lines.append('\\\\caption{Multi-Seed Robustness Summary (mean±sd; 95\\% CI)}')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Export LaTeX table summarizing mean±sd and 95% CI\n",
    "if len(res)>0:\n",
    "    def fmt(m,sd,lo,hi):\n",
    "        if np.isnan(m): return '--'\n",
    "        return f\"{m:.3f} ± {sd:.3f} (CI {lo:.3f},{hi:.3f})\"\n",
    "    tbl = agg_df.pivot(index='metric', columns='alg', values=['mean','sd','ci_low','ci_high'])\n",
    "    lines = []\n",
    "    lines.append('\\\\begin{table}[t]')\n",
    "    lines.append('\\\\centering')\n",
    "    lines.append('\\\\caption{Multi-Seed Robustness Summary (mean±sd; 95\\% CI)}')\n",
    "    lines.append('\\\\begin{tabular}{lcc}')\n",
    "    lines.append('\\\\toprule')\n",
    "    lines.append('Metric & PETS & MBPO \\\\')\n",
    "    lines.append('\\\\midrule')\n",
    "    for metric in ['time_to_mastery','post_content_gain','cum_reward','reward_var','blueprint_dev']:\n",
    "        row = []\n",
    "        row.append(metric.replace('_','\\\\_'))\n",
    "        for alg in ['PETS','MBPO']:\n",
    "            try:\n",
    "                m = tbl['mean'][alg][metric]; sd = tbl['sd'][alg][metric]; lo = tbl['ci_low'][alg][metric]; hi = tbl['ci_high'][alg][metric]\n",
    "                row.append(fmt(m,sd,lo,hi))\n",
    "            except Exception:\n",
    "                row.append('--')\n",
    "        lines.append(' & '.join(row) + ' \\\\')\n",
    "    lines.append('\\\\bottomrule')\n",
    "    lines.append('\\\\end{tabular}')\n",
    "    lines.append('\\\\label{tab:robustness}')\n",
    "    lines.append('\\\\end{table}')\n",
    "    tex = '\\n'.join(lines)\n",
    "    with open('/mnt/data/robustness_table.tex','w') as f:\n",
    "        f.write(tex)\n",
    "    print('Wrote LaTeX table to /mnt/data/robustness_table.tex')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
